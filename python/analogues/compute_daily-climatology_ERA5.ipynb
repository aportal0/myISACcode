{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'msl'\n",
    "varstr = 'mslp'\n",
    "n_window = 31\n",
    "data_dir = '/work_big/users/clima/portal/ERA5/'+varstr+'/'\n",
    "year_range = [2004, 2023]   # for reference climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_2005.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_2006.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_2007.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_2008.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_2009.nc']\n",
      "['/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_1985.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_1986.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_1987.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_1988.nc', '/work_big/users/clima/portal/ERA5/mslp/ERA5_mslp_NH_daily_1989.nc']\n"
     ]
    }
   ],
   "source": [
    "# Define the file pattern\n",
    "file_pattern = data_dir + \"ERA5_\"+varstr+\"_NH_daily_????.nc\"\n",
    "# Get all file paths matching the pattern\n",
    "all_files = glob.glob(file_pattern)\n",
    "# Select and sort files\n",
    "selected_files = []\n",
    "for year in range(year_range[0], year_range[1]+1):\n",
    "    selected_files += [file for file in all_files if str(year) in file]\n",
    "selected_files = sorted(selected_files)\n",
    "print(selected_files[:5])\n",
    "print(sorted(all_files)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and compute climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Every dimension requires a corresponding 1D coordinate and index for inferring concatenation order but the coordinate 'longitude' has no corresponding index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds_clim\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Upload data and compute the daily climatology\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m data_daily \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselected_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mby_coords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m clim_daily \u001b[38;5;241m=\u001b[39m daily_clim(data_daily)\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save the daily climatology\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geo_env/lib/python3.13/site-packages/xarray/backends/api.py:1606\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     combined \u001b[38;5;241m=\u001b[39m _nested_combine(\n\u001b[1;32m   1594\u001b[0m         datasets,\n\u001b[1;32m   1595\u001b[0m         concat_dims\u001b[38;5;241m=\u001b[39mconcat_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m   1602\u001b[0m     )\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby_coords\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;66;03m# Redo ordering from coordinates, ignoring how they were ordered\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;66;03m# previously\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_by_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1616\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is an invalid option for the keyword argument\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ``combine``\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1618\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/geo_env/lib/python3.13/site-packages/xarray/core/combine.py:961\u001b[0m, in \u001b[0;36mcombine_by_coords\u001b[0;34m(data_objects, compat, data_vars, coords, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     grouped_by_vars \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mgroupby(sorted_datasets, key\u001b[38;5;241m=\u001b[39mvars_as_keys)\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;66;03m# Perform the multidimensional combine on each group of data variables\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# before merging back together\u001b[39;00m\n\u001b[0;32m--> 961\u001b[0m     concatenated_grouped_by_data_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrouped_by_vars\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m    975\u001b[0m     concatenated_grouped_by_data_vars,\n\u001b[1;32m    976\u001b[0m     compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m     combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    980\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/geo_env/lib/python3.13/site-packages/xarray/core/combine.py:962\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    957\u001b[0m     grouped_by_vars \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mgroupby(sorted_datasets, key\u001b[38;5;241m=\u001b[39mvars_as_keys)\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;66;03m# Perform the multidimensional combine on each group of data variables\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# before merging back together\u001b[39;00m\n\u001b[1;32m    961\u001b[0m     concatenated_grouped_by_data_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 962\u001b[0m         \u001b[43m_combine_single_variable_hypercube\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets_with_same_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mvars\u001b[39m, datasets_with_same_vars \u001b[38;5;129;01min\u001b[39;00m grouped_by_vars\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m    975\u001b[0m     concatenated_grouped_by_data_vars,\n\u001b[1;32m    976\u001b[0m     compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m     combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    980\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/geo_env/lib/python3.13/site-packages/xarray/core/combine.py:622\u001b[0m, in \u001b[0;36m_combine_single_variable_hypercube\u001b[0;34m(datasets, fill_value, data_vars, coords, compat, join, combine_attrs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(datasets) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one Dataset is required to resolve variable names \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor combined hypercube.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m     )\n\u001b[0;32m--> 622\u001b[0m combined_ids, concat_dims \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_concat_order_from_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;66;03m# check that datasets form complete hypercube\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     _check_shape_tile_ids(combined_ids)\n",
      "File \u001b[0;32m~/anaconda3/envs/geo_env/lib/python3.13/site-packages/xarray/core/combine.py:97\u001b[0m, in \u001b[0;36m_infer_concat_order_from_coords\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m indexes):\n\u001b[1;32m     92\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvery dimension requires a corresponding 1D coordinate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand index for inferring concatenation order but the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinate \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no corresponding index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# TODO (benbovy, flexible indexes): support flexible indexes?\u001b[39;00m\n\u001b[1;32m    100\u001b[0m indexes \u001b[38;5;241m=\u001b[39m [index\u001b[38;5;241m.\u001b[39mto_pandas_index() \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m indexes]\n",
      "\u001b[0;31mValueError\u001b[0m: Every dimension requires a corresponding 1D coordinate and index for inferring concatenation order but the coordinate 'longitude' has no corresponding index"
     ]
    }
   ],
   "source": [
    "# Define postprocessing function\n",
    "def daily_clim(ds, var_name=varname):\n",
    "    ds_clim = ds.groupby(\"time.dayofyear\").mean(\"time\")[var_name]\n",
    "    return ds_clim\n",
    "\n",
    "# Upload data and compute the daily climatology\n",
    "data_daily = xr.open_mfdataset(\n",
    "    selected_files, \\\n",
    "    use_cftime=True, \\\n",
    "    combine='by_coords', \\\n",
    "    chunks={'longitude': -1, 'lat': 10,'time': -1}, \\\n",
    "    )\n",
    "clim_daily = daily_clim(data_daily).compute()\n",
    "\n",
    "# Save the daily climatology\n",
    "clim_daily.to_netcdf(data_dir + 'climatology/ERA5_'+varstr+'_NH_daily_clim_'+str(year_range[0])+'-'+str(year_range[1])+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend and smooth the daily climatology with a n_window running mean\n",
    "n_days = np.floor(n_window / 2).astype(int)\n",
    "clim_extended = xr.concat(\n",
    "    [clim_daily[-n_days:], clim_daily, clim_daily[:n_days]],\n",
    "    dim=\"dayofyear\"\n",
    ")\n",
    "new_dayofyear = np.arange(-n_days + 1, 365 + n_days + 1)\n",
    "clim_extended = clim_extended.assign_coords(dayofyear=new_dayofyear)\n",
    "clim_smooth = clim_extended.rolling(dayofyear=n_window, center=True).mean()\n",
    "clim_smooth = clim_smooth.sel(dayofyear=slice(1, 365))\n",
    "\n",
    "# Save the smoothed climatology\n",
    "clim_smooth.to_netcdf(data_dir + 'climatology/ERA5_'+varstr+'_NH_daily_clim_'+str(year_range[0])+'-'+str(year_range[1])+'_sm'+str(n_window)+'d.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute anomalies from climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  ERA5_z500_NH_daily_1985_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1986_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1987_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1988_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1989_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1990_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1991_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1992_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1993_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1994_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1995_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1996_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1997_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1998_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_1999_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2000_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2001_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2002_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2003_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2004_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2005_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2006_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2007_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2008_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2009_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2010_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2011_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2012_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2013_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2014_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2015_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2016_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2017_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2018_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2019_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2020_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2021_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2022_anom.nc is saved\n",
      "File  ERA5_z500_NH_daily_2023_anom.nc is saved\n"
     ]
    }
   ],
   "source": [
    "for file in sorted(all_files):\n",
    "    data = xr.open_dataset(file)\n",
    "    clim = xr.open_dataset(data_dir + 'climatology/ERA5_'+varstr+'_NH_daily_clim_'+str(year_range[0])+'-'+str(year_range[1])+'_sm'+str(n_window)+'d.nc')\n",
    "    clim = clim.sel(dayofyear=data['time'].dt.dayofyear).drop_vars(\"dayofyear\")\n",
    "    anom = data - clim\n",
    "    anom.to_netcdf(data_dir + file.split('/')[-1].split('.')[0] + '_anom.nc')\n",
    "    print('File ', file.split('/')[-1].split('.')[0] + '_anom.nc is saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
